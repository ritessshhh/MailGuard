{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHC-l8exS9pA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def read_file(path):\n",
        "    \"\"\"\n",
        "    read and return all data in a file\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        return f.read()\n",
        "\n",
        "def load_data():\n",
        "    # load all data from file\n",
        "    data_path = \"data/SpamDetectionData.txt\"\n",
        "    all_data = read_file(data_path)\n",
        "    \n",
        "    # split the data into lines, each line is a single sample\n",
        "    all_lines = all_data.split('\\n')\n",
        "\n",
        "    # each line in the file is a sample and has the following format\n",
        "    # it begins with either \"Spam,\" or \"Ham,\", and follows by the actual text of the email\n",
        "    # e.g. Spam,<p>His honeyed and land....\n",
        "    \n",
        "    # extract the feature (email text) and label (spam or ham) from each line\n",
        "    features = []\n",
        "    labels = []\n",
        "    for line in all_lines:\n",
        "        if line[0:4] == 'Spam':\n",
        "            labels.append(1)\n",
        "            features.append(line[5:])\n",
        "            pass\n",
        "        elif line[0:3] == 'Ham':\n",
        "            labels.append(0)\n",
        "            features.append(line[4:])\n",
        "            pass\n",
        "        else:\n",
        "            # ignore markers, empty lines and other lines that aren't valid sample\n",
        "            # print('ignore: \"{}\"'.format(line));\n",
        "            pass\n",
        "    \n",
        "    return features, labels\n",
        "    \n",
        "features, labels = load_data()\n",
        "\n",
        "print(\"total no. of samples: {}\".format(len(labels)))\n",
        "print(\"total no. of spam samples: {}\".format(labels.count(1)))\n",
        "print(\"total no. of ham samples: {}\".format(labels.count(0)))\n",
        "\n",
        "print(\"\\nPrint a random sample for inspection:\")\n",
        "random_idx = random.randint(0, len(labels))\n",
        "print(\"example feature: {}\".format(features[random_idx][0:]))\n",
        "print(\"example label: {} ({})\".format(labels[random_idx], 'spam' if labels[random_idx] else 'ham'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load features and labels\n",
        "features, labels = load_data()\n",
        "\n",
        "# split data into training / test sets\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, \n",
        "    labels, \n",
        "    test_size=0.2,   # use 10% for testing\n",
        "    random_state=42)\n",
        "\n",
        "print(\"no. of train features: {}\".format(len(features_train)))\n",
        "print(\"no. of train labels: {}\".format(len(labels_train)))\n",
        "print(\"no. of test features: {}\".format(len(features_test)))\n",
        "print(\"no. of test labels: {}\".format(len(labels_test)))"
      ],
      "metadata": {
        "id": "ALeVbAT8TC-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorize email text into tfidf matrix\n",
        "# TfidfVectorizer converts collection of raw documents to a matrix of TF-IDF features.\n",
        "# It's equivalent to CountVectorizer followed by TfidfTransformer.\n",
        "vectorizer = TfidfVectorizer(\n",
        "    input='content',     # input is actual text\n",
        "    lowercase=True,      # convert to lower case before tokenizing\n",
        "    stop_words='english' # remove stop words\n",
        ")\n",
        "features_train_transformed = vectorizer.fit_transform(features_train)\n",
        "features_test_transformed  = vectorizer.transform(features_test)"
      ],
      "metadata": {
        "id": "pbwWyMl9TDv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pickle\n",
        "\n",
        "def save(vectorizer, classifier):\n",
        "    '''\n",
        "    save classifier to disk\n",
        "    '''\n",
        "    with open('model.pkl', 'wb') as file:\n",
        "        pickle.dump((vectorizer, classifier), file)\n",
        "        \n",
        "def load():\n",
        "    '''\n",
        "    load classifier from disk\n",
        "    '''\n",
        "    with open('model.pkl', 'rb') as file:\n",
        "      vectorizer, classifier = pickle.load(file)\n",
        "    return vectorizer, classifier\n",
        "\n",
        "# train a classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(features_train_transformed, labels_train)\n",
        "\n",
        "# save the trained model\n",
        "save(vectorizer, classifier)\n",
        "\n",
        "# score the classifier accuracy\n",
        "print(\"classifier accuracy {:.2f}%\".format(classifier.score(features_test_transformed, labels_test) * 100))"
      ],
      "metadata": {
        "id": "OIAA2GlwTIDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "prediction = classifier.predict(features_test_transformed)\n",
        "fscore = metrics.f1_score(labels_test, prediction, average='macro')\n",
        "print(\"F score {:.2f}\".format(fscore))"
      ],
      "metadata": {
        "id": "rOFM0BvTTKSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import os.path\n",
        "import base64\n",
        "import re\n",
        "\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']"
      ],
      "metadata": {
        "id": "g_wXFEpUHJHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Shows basic usage of the Gmail API.\n",
        "    Lists the user's Gmail labels.\n",
        "    \"\"\"\n",
        "    creds = None\n",
        "    # The file token.json stores the user's access and refresh tokens, and is\n",
        "    # created automatically when the authorization flow completes for the first\n",
        "    # time.\n",
        "    if os.path.exists('token.json'):\n",
        "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
        "    # If there are no (valid) credentials available, let the user log in.\n",
        "    if not creds or not creds.valid:\n",
        "        if creds and creds.expired and creds.refresh_token:\n",
        "            creds.refresh(Request())\n",
        "        else:\n",
        "            flow = InstalledAppFlow.from_client_secrets_file(\n",
        "                'credentials.json', SCOPES)\n",
        "            creds = flow.run_local_server(port=0)\n",
        "        # Save the credentials for the next run\n",
        "        with open('token.json', 'w') as token:\n",
        "            token.write(creds.to_json())\n",
        "\n",
        "    try:\n",
        "        service = build('gmail', 'v1', credentials=creds)\n",
        "        result = service.users().messages().list(userId='me').execute()\n",
        "\n",
        "        messages = result.get('messages')\n",
        "\n",
        "        for msg in messages:\n",
        "\n",
        "            txt = service.users().messages().get(userId='me', id=msg['id']).execute()\n",
        "            try:\n",
        "\n",
        "                payload = txt['payload']\n",
        "                headers = payload['headers']\n",
        "\n",
        "                for d in headers:\n",
        "                    if d['name'] == 'Subject':\n",
        "                        subject = d['value']\n",
        "                    if d['name'] == 'From':\n",
        "                        sender = d['value']\n",
        "\n",
        "                parts = payload.get('parts')[0]\n",
        "                data = parts['body']['data']\n",
        "                data = data.replace(\"-\", \"+\").replace(\"_\", \"/\")\n",
        "                text = base64.b64decode(data)\n",
        "\n",
        "                text = text.decode('utf-8')\n",
        "                text = re.sub(r'[\\r\\n\\xe2\\x80\\x99]', ' ', text)\n",
        "\n",
        "                print(\"Subject: \", subject)\n",
        "                print(\"From: \", sender)\n",
        "                print(\"Message: \", text)\n",
        "                print('\\n')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "    except HttpError as error:\n",
        "        # TODO(developer) - Handle errors from gmail API.\n",
        "        print(f'An error occurred: {error}')"
      ],
      "metadata": {
        "id": "u6ytXFCoHJ8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer, classifer = load()\n",
        "\n",
        "print('\\nPerform a test')                    \n",
        "#email_input = 'enter your email here'\n",
        "if __name__ == '__main__':\n",
        "   email_input = main()\n",
        "   \n",
        "email_input_transformed = vectorizer.transform(email_input)\n",
        "prediction = classifer.predict(email_input_transformed)\n",
        "\n",
        "print('EMAIL:', email_input)\n",
        "print('The email is', 'SPAM' if prediction else 'HAM')"
      ],
      "metadata": {
        "id": "m256b2SWTMbj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}